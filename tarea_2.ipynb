{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import huffman\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "import tensorflow_datasets as tfds\n",
    "import glob\n",
    "import os\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"este es un ejemplo de codificación de Huffman\"\n",
    "frequencies = Counter(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto codificado: 110 || 0100 || 111101 || 110 || 101 || 110 || 0100 || 101 || 0010 || 0111 || 101 || 110 || 111100 || 110 || 0011 || 00010 || 01010 || 0000 || 101 || 1110 || 110 || 101 || 0110 || 0000 || 1110 || 1001 || 1000 || 1001 || 0110 || 11111 || 0110 || 1001 || 00011 || 0111 || 101 || 1110 || 110 || 101 || 01011 || 0010 || 1000 || 1000 || 0011 || 11111 || 0111\n",
      "Texto decodificado: e\n"
     ]
    }
   ],
   "source": [
    "codigo_huffman = huffman.codebook(frequencies.items())\n",
    "texto_codificado = \" || \".join(codigo_huffman[char] for char in texto)\n",
    "\n",
    "print(\"Texto codificado:\", texto_codificado)\n",
    "\n",
    "# Paso 4: Decodificar el texto codificado\n",
    "# Crear un diccionario inverso para decodificar\n",
    "codigo_inverso = {v: k for k, v in codigo_huffman.items()}\n",
    "texto_decodificado = \"\"\n",
    "temp = \"\"\n",
    "for bit in texto_codificado:\n",
    "    temp += bit\n",
    "    if temp in codigo_inverso:\n",
    "        texto_decodificado += codigo_inverso[temp]\n",
    "        temp = \"\"\n",
    "\n",
    "print(\"Texto decodificado:\", texto_decodificado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, seq_length=40):\n",
    "    chars = sorted(set(text))\n",
    "    char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "    idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "    # Crear secuencias de entrada y salida\n",
    "    input_seq = []\n",
    "    target_seq = []\n",
    "\n",
    "    for i in range(len(text) - seq_length):\n",
    "        in_seq = text[i : i + seq_length]\n",
    "        out_seq = text[i + seq_length]\n",
    "        input_seq.append([char_to_idx[char] for char in in_seq])\n",
    "        target_seq.append(char_to_idx[out_seq])\n",
    "\n",
    "    # Convertir listas de índices en matrices de entrada y salida adecuadas para el entrenamiento\n",
    "    input_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        input_seq, maxlen=seq_length, padding=\"pre\"\n",
    "    )\n",
    "    target_seq = tf.keras.utils.to_categorical(target_seq, num_classes=vocab_size)\n",
    "\n",
    "    return input_seq, target_seq, vocab_size, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape: [[ 1  0  4 ... 15  8 16]\n",
      " [ 0  4  1 ...  8 16  1]\n",
      " [ 4  1  7 ... 16  1 22]\n",
      " ...\n",
      " [ 1 14  9 ... 18 18  5]\n",
      " [14  9  1 ... 18  5  6]\n",
      " [ 9  1  5 ...  5  6  5]]\n",
      "Target sequence shape: [[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "Vocabulary size: {0: '\\n', 1: ' ', 2: 'A', 3: 'P', 4: 'Y', 5: 'a', 6: 'b', 7: 'c', 8: 'd', 9: 'e', 10: 'g', 11: 'h', 12: 'i', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'z', 24: 'á', 25: 'é', 26: 'í'}\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcofura/anaconda3/envs/fundamentos_matematicos/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 - 1s - 132ms/step - accuracy: 0.1618 - loss: 3.2793\n",
      "Epoch 2/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.1863 - loss: 3.1925\n",
      "Epoch 3/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.1863 - loss: 2.9225\n",
      "Epoch 4/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.1912 - loss: 2.8323\n",
      "Epoch 5/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.1422 - loss: 2.7818\n",
      "Epoch 6/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.2157 - loss: 2.7413\n",
      "Epoch 7/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.1863 - loss: 2.7347\n",
      "Epoch 8/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.1863 - loss: 2.7149\n",
      "Epoch 9/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.1912 - loss: 2.6929\n",
      "Epoch 10/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.1961 - loss: 2.6745\n",
      "Epoch 11/100\n",
      "7/7 - 0s - 13ms/step - accuracy: 0.1961 - loss: 2.6526\n",
      "Epoch 12/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.1961 - loss: 2.6338\n",
      "Epoch 13/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.2451 - loss: 2.5988\n",
      "Epoch 14/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.2500 - loss: 2.5682\n",
      "Epoch 15/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.2941 - loss: 2.5319\n",
      "Epoch 16/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.3088 - loss: 2.4894\n",
      "Epoch 17/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.3039 - loss: 2.4495\n",
      "Epoch 18/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.3333 - loss: 2.3798\n",
      "Epoch 19/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.3578 - loss: 2.3306\n",
      "Epoch 20/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.3676 - loss: 2.2633\n",
      "Epoch 21/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.3922 - loss: 2.2106\n",
      "Epoch 22/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.3725 - loss: 2.1578\n",
      "Epoch 23/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.4069 - loss: 2.0736\n",
      "Epoch 24/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.4167 - loss: 2.0253\n",
      "Epoch 25/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.4216 - loss: 1.9771\n",
      "Epoch 26/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.4314 - loss: 1.8975\n",
      "Epoch 27/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.4804 - loss: 1.8377\n",
      "Epoch 28/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.4853 - loss: 1.7812\n",
      "Epoch 29/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.4853 - loss: 1.7178\n",
      "Epoch 30/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.5245 - loss: 1.6404\n",
      "Epoch 31/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.5196 - loss: 1.5937\n",
      "Epoch 32/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.5147 - loss: 1.5406\n",
      "Epoch 33/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.5539 - loss: 1.4793\n",
      "Epoch 34/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.5833 - loss: 1.4181\n",
      "Epoch 35/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.6127 - loss: 1.3527\n",
      "Epoch 36/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.6029 - loss: 1.2925\n",
      "Epoch 37/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.6078 - loss: 1.2499\n",
      "Epoch 38/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.6618 - loss: 1.2247\n",
      "Epoch 39/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.6618 - loss: 1.1522\n",
      "Epoch 40/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.7304 - loss: 1.0821\n",
      "Epoch 41/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.7451 - loss: 1.0135\n",
      "Epoch 42/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.7255 - loss: 0.9525\n",
      "Epoch 43/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.7990 - loss: 0.9007\n",
      "Epoch 44/100\n",
      "7/7 - 0s - 15ms/step - accuracy: 0.7843 - loss: 0.8587\n",
      "Epoch 45/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.8333 - loss: 0.7930\n",
      "Epoch 46/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.8480 - loss: 0.7196\n",
      "Epoch 47/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.8676 - loss: 0.6747\n",
      "Epoch 48/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.8775 - loss: 0.6248\n",
      "Epoch 49/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.8725 - loss: 0.5987\n",
      "Epoch 50/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9363 - loss: 0.5435\n",
      "Epoch 51/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.9363 - loss: 0.4977\n",
      "Epoch 52/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9608 - loss: 0.4630\n",
      "Epoch 53/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9657 - loss: 0.4255\n",
      "Epoch 54/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9804 - loss: 0.3951\n",
      "Epoch 55/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9706 - loss: 0.3769\n",
      "Epoch 56/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9657 - loss: 0.3671\n",
      "Epoch 57/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9853 - loss: 0.3156\n",
      "Epoch 58/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.9951 - loss: 0.2924\n",
      "Epoch 59/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.9902 - loss: 0.2699\n",
      "Epoch 60/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9951 - loss: 0.2432\n",
      "Epoch 61/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.9902 - loss: 0.2288\n",
      "Epoch 62/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9902 - loss: 0.2154\n",
      "Epoch 63/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9951 - loss: 0.1990\n",
      "Epoch 64/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9951 - loss: 0.1811\n",
      "Epoch 65/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9951 - loss: 0.1661\n",
      "Epoch 66/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.1540\n",
      "Epoch 67/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.1433\n",
      "Epoch 68/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 1.0000 - loss: 0.1316\n",
      "Epoch 69/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.1232\n",
      "Epoch 70/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.1153\n",
      "Epoch 71/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.1079\n",
      "Epoch 72/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.1018\n",
      "Epoch 73/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0957\n",
      "Epoch 74/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0914\n",
      "Epoch 75/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0869\n",
      "Epoch 76/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0828\n",
      "Epoch 77/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0796\n",
      "Epoch 78/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0757\n",
      "Epoch 79/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0712\n",
      "Epoch 80/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0678\n",
      "Epoch 81/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0653\n",
      "Epoch 82/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0626\n",
      "Epoch 83/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0596\n",
      "Epoch 84/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0573\n",
      "Epoch 85/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0547\n",
      "Epoch 86/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0527\n",
      "Epoch 87/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0502\n",
      "Epoch 88/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0488\n",
      "Epoch 89/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0469\n",
      "Epoch 90/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0457\n",
      "Epoch 91/100\n",
      "7/7 - 0s - 14ms/step - accuracy: 1.0000 - loss: 0.0440\n",
      "Epoch 92/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0424\n",
      "Epoch 93/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0414\n",
      "Epoch 94/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0396\n",
      "Epoch 95/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0385\n",
      "Epoch 96/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 1.0000 - loss: 0.0374\n",
      "Epoch 97/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 1.0000 - loss: 0.0362\n",
      "Epoch 98/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0348\n",
      "Epoch 99/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0344\n",
      "Epoch 100/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3130e3590>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de ejemplo\n",
    "text = \"\"\" Hola, ¿cómo estás? Soy un ejemplo de texto que se utilizará para entrenar un modelo de lenguaje.\n",
    "\"\"\"  \n",
    "seq_length = 10  \n",
    "input_seq, target_seq, vocab_size, char_to_idx, idx_to_char = preprocess_text(\n",
    "    text, seq_length\n",
    ")\n",
    "print(\"Input sequence shape:\", input_seq)\n",
    "print(\"Target sequence shape:\", target_seq)\n",
    "print(\"Vocabulary size:\", idx_to_char)\n",
    "# Construir el modelo\n",
    "model = Sequential(\n",
    "    [\n",
    "        Embedding(input_dim=vocab_size, output_dim=64, input_length=seq_length),\n",
    "        LSTM(128),\n",
    "        Dense(vocab_size, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(input_seq, target_seq, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "{'\\n': 2.4446226e-06, ' ': 0.00012292077, 'A': 3.9709444e-06, 'P': 0.00018478041, 'Y': 0.00021594099, 'a': 0.002025231, 'b': 0.0045404113, 'c': 2.0484175e-07, 'd': 4.2705764e-05, 'e': 0.9816928, 'g': 2.1880491e-07, 'h': 2.7613967e-05, 'i': 0.000121175595, 'l': 0.0043668956, 'm': 0.0023769338, 'n': 2.608391e-05, 'o': 5.560564e-06, 'q': 2.5820671e-05, 'r': 0.0002562776, 's': 0.0029980824, 't': 2.9368462e-05, 'u': 0.00015539383, 'v': 0.00076011376, 'z': 7.896544e-07, 'á': 5.8225937e-06, 'é': 1.0626007e-05, 'í': 1.7690279e-06}\n"
     ]
    }
   ],
   "source": [
    "def predict_next_char(model, input_text, char_to_idx, idx_to_char):\n",
    "    input_indices = [\n",
    "        char_to_idx[ch] for ch in input_text[-seq_length:]\n",
    "    ]  # Asegurarse de usar solo los últimos 'seq_length' caracteres\n",
    "    input_indices = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [input_indices], maxlen=seq_length, padding=\"pre\"\n",
    "    )\n",
    "    predictions = model.predict(input_indices)[0]\n",
    "    return {idx_to_char[i]: prob for i, prob in enumerate(predictions)}\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "context = \"[]Hola\"\n",
    "predicted_probs = predict_next_char(model, context, char_to_idx, idx_to_char)\n",
    "print(predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),categories=['alt.atheism'])\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),categories=['alt.atheism'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 319)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_train.data), len(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts(directory):\n",
    "    texts = []\n",
    "    # Recorre todos los archivos en el directorio\n",
    "    for filename in glob.glob(os.path.join(directory, \"*\")):\n",
    "        if os.path.isfile(filename):  # Check if the path is a file\n",
    "            with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "                texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Suponiendo que '20news-bydate-train' es tu directorio descomprimido\n",
    "train_dir = \"./20news-bydate/20news-bydate-train/alt.atheism\"\n",
    "test_dir = \"./20news-bydate/20news-bydate-test/alt.atheism\"\n",
    "texts = load_texts(train_dir)\n",
    "texts_test = load_texts(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(texts, seq_length=40):\n",
    "    input_chars = []\n",
    "    output_char = []\n",
    "    for text in texts:\n",
    "        for i in range(len(text) - seq_length):\n",
    "            in_seq = text[i:i + seq_length]\n",
    "            out_seq = text[i + seq_length]\n",
    "            input_chars.append(in_seq)\n",
    "            output_char.append(out_seq)\n",
    "    return input_chars, output_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 20\n",
    "texts_train_part = newsgroups_train.data[:100]\n",
    "texts_test_part = newsgroups_test.data[:25]\n",
    "input_seqs, output_seqs = create_sequences(texts_train_part, seq_length)\n",
    "input_seqs_test, output_seqs_test = create_sequences(texts_test_part, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89538"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172544"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcofura/anaconda3/envs/fundamentos_matematicos/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_52\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_52\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_110 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_111 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_48 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_110 (\u001b[38;5;33mLSTM\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_65 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_111 (\u001b[38;5;33mLSTM\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_71 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_72 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 105ms/step - accuracy: 0.2047 - loss: 3.0392 - val_accuracy: 0.4048 - val_loss: 2.2121\n",
      "Epoch 2/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 105ms/step - accuracy: 0.3816 - loss: 2.1994 - val_accuracy: 0.4769 - val_loss: 1.9456\n",
      "Epoch 3/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 104ms/step - accuracy: 0.4563 - loss: 1.9095 - val_accuracy: 0.5153 - val_loss: 1.8083\n",
      "Epoch 4/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 102ms/step - accuracy: 0.5050 - loss: 1.7053 - val_accuracy: 0.5302 - val_loss: 1.7447\n",
      "Epoch 5/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 104ms/step - accuracy: 0.5349 - loss: 1.5898 - val_accuracy: 0.5446 - val_loss: 1.7153\n",
      "Epoch 6/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 109ms/step - accuracy: 0.5595 - loss: 1.4895 - val_accuracy: 0.5536 - val_loss: 1.7267\n",
      "Epoch 7/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 114ms/step - accuracy: 0.5811 - loss: 1.4052 - val_accuracy: 0.5588 - val_loss: 1.7368\n",
      "Epoch 8/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 120ms/step - accuracy: 0.5938 - loss: 1.3503 - val_accuracy: 0.5614 - val_loss: 1.7630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x4e1fd3bc0>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Tokenización\n",
    "tokenizer = Tokenizer(char_level=True,lower=False)\n",
    "tokenizer.fit_on_texts(newsgroups_train.data)  # Ajuste del tokenizer al corpus completo\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convertir textos a secuencias numéricas\n",
    "input_seqs = tokenizer.texts_to_sequences(input_seqs)\n",
    "input_seqs = pad_sequences(input_seqs, maxlen=seq_length, truncating='pre')\n",
    "\n",
    "# Preparar datos de salida (etiquetas) usando one-hot encoding\n",
    "output_seqs = tokenizer.texts_to_sequences(output_seqs)\n",
    "output_seqs = to_categorical(output_seqs, num_classes=vocab_size)\n",
    "\n",
    "# Test\n",
    "input_seqs_test = tokenizer.texts_to_sequences(input_seqs_test)\n",
    "input_seqs_test = pad_sequences(input_seqs_test, maxlen=seq_length, truncating='pre')\n",
    "\n",
    "# Preparar datos de salida (etiquetas) usando one-hot encoding\n",
    "output_seqs_test = tokenizer.texts_to_sequences(output_seqs_test)\n",
    "output_seqs_test = to_categorical(output_seqs_test, num_classes=vocab_size)\n",
    "\n",
    "# Construcción del modelo LSTM\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=50, input_length=seq_length),\n",
    "    LSTM(512,return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(256),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model.fit(input_seqs, output_seqs, epochs=50, batch_size=64,validation_data=(input_seqs_test, output_seqs_test),callbacks=[early_stopping,model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, tokenizer, input_text, seq_length):\n",
    "    # Paso 1: Convertir la cadena de entrada en una secuencia de enteros\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    \n",
    "    # Paso 2: Ajustar la longitud de la secuencia\n",
    "    input_seq = pad_sequences([input_seq], maxlen=seq_length, truncating='pre')\n",
    "    \n",
    "    # Paso 3: Hacer la predicción\n",
    "    prediction = model.predict(input_seq, verbose=0)[0]\n",
    "    \n",
    "    # Paso 4: Convertir las probabilidades en un diccionario\n",
    "    int_to_char = {i: char for char, i in tokenizer.word_index.items()}\n",
    "    int_to_char[0] = ''  # Añadir el carácter vacío para el padding\n",
    "    return {int_to_char[i]: prob for i, prob in enumerate(prediction)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 2.912069e-10, ' ': 0.008098608, 'e': 0.84537244, 't': 0.0050773835, 'o': 0.015000976, 'a': 0.007953188, 'i': 0.062609546, 'n': 0.0002290401, 's': 0.002169434, 'r': 0.00017455738, 'h': 0.004103605, 'l': 0.0011149038, '\\n': 0.0007341459, 'd': 1.9611709e-05, 'u': 0.0057491045, 'c': 8.3980376e-05, 'm': 0.0004142076, '.': 0.0015490736, 'y': 0.036456842, 'f': 9.44468e-05, 'g': 3.0248479e-05, 'p': 5.917111e-06, 'w': 6.38095e-05, 'b': 0.000103438884, '>': 2.8584232e-08, 'v': 3.9760475e-06, ',': 0.0011134631, '-': 8.900999e-05, 'I': 4.9225747e-07, 'k': 2.7681324e-05, ':': 4.7664907e-06, 'T': 2.6036204e-09, \"'\": 6.734473e-05, '\"': 9.335563e-05, '*': 5.7025777e-06, 'A': 1.3488265e-07, 'S': 1.1493323e-08, ')': 0.00069851323, '(': 8.5612805e-07, '1': 1.2899555e-07, 'C': 6.7450685e-08, '@': 7.252227e-05, 'R': 1.5494509e-08, 'x': 4.506911e-07, 'M': 2.4733743e-10, 'O': 1.575933e-08, 'N': 4.1148773e-10, 'B': 1.151856e-08, 'j': 8.361986e-07, '^': 3.3197698e-09, 'P': 8.320143e-11, '?': 0.0002837003, 'L': 1.3808361e-08, '0': 4.844088e-06, 'F': 2.1106328e-09, 'W': 5.779341e-08, 'H': 1.8109017e-07, 'E': 5.3258038e-08, '|': 2.2344184e-06, 'D': 7.380165e-09, '2': 4.8732694e-08, 'G': 4.1150617e-08, 'z': 1.8309662e-05, '3': 6.4563257e-09, '=': 4.2119723e-08, '9': 1.1567393e-08, 'q': 6.1634346e-08, '/': 6.6981843e-06, 'U': 1.67524e-09, '5': 1.392513e-08, 'J': 1.251009e-10, '4': 2.0460734e-06, '#': 9.8085325e-12, 'K': 1.034926e-09, '_': 7.934327e-06, '6': 1.1138223e-08, '<': 4.1235708e-08, '\\\\': 1.203758e-07, 'Y': 1.0800841e-06, '8': 3.7995343e-07, '7': 2.951099e-07, '!': 0.00024726486, '\\t': 3.5430926e-07, '[': 1.4580877e-10, ']': 1.4681714e-05, ';': 1.7560335e-05, 'V': 1.53697e-09, 'Q': 2.5223762e-12, '+': 1.9225176e-07, '~': 2.7078262e-10, 'X': 4.0748787e-07, '$': 1.0914003e-06, 'Z': 1.9528986e-13, '`': 2.2017202e-10, '&': 2.1146616e-06, '%': 2.2891232e-08, '}': 1.9484518e-10, '{': 3.533295e-10, '\\x1b': 1.9966599e-10, '\\x0c': 2.6998226e-10, '\\x10': 2.4353905e-10}\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "input_text = \"I am not familiar with, or knowledgeable about the original languag\"\n",
    "predictions = predict_next_char(model, tokenizer, input_text, seq_length)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['', ' ', 'e', 't', 'o', 'a', 'i', 'n', 's', 'r', 'h', 'l', '\\n', 'd', 'u', 'c', 'm', '.', 'y', 'f', 'g', 'p', 'w', 'b', '>', 'v', ',', '-', 'I', 'k', ':', 'T', \"'\", '\"', '*', 'A', 'S', ')', '(', '1', 'C', '@', 'R', 'x', 'M', 'O', 'N', 'B', 'j', '^', 'P', '?', 'L', '0', 'F', 'W', 'H', 'E', '|', 'D', '2', 'G', 'z', '3', '=', '9', 'q', '/', 'U', '5', 'J', '4', '#', 'K', '_', '6', '<', '\\\\', 'Y', '8', '7', '!', '\\t', '[', ']', ';', 'V', 'Q', '+', '~', 'X', '$', 'Z', '`', '&', '%', '}', '{', '\\x1b', '\\x0c', '\\x10'])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key con mayor valor: e\n"
     ]
    }
   ],
   "source": [
    "max_key = max(predictions, key=predictions.get)\n",
    "print(\"Key con mayor valor:\", max_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar texto de entrada \n",
    "def codificar_texto(texto:str):\n",
    "    text_encoded = []\n",
    "    for i in range(len(texto)):\n",
    "        input_text = ''\n",
    "        if i>0:\n",
    "            input_text=texto[:i]\n",
    "        print('Texto a predecir:',input_text)\n",
    "        predictions = predict_next_char(model, tokenizer, input_text, seq_length)\n",
    "        max_key = max(predictions, key=predictions.get)\n",
    "        print('Siguiente caracter más probable:',max_key)\n",
    "        symbolweights = [(char, weight) for char, weight in predictions.items()]\n",
    "        codigo_huffman = huffman.codebook(symbolweights)\n",
    "        print('Caracter a codificar:',texto[i])\n",
    "        texto_codificado =codigo_huffman[texto[i]]\n",
    "        print('Texto codificado:',texto_codificado)\n",
    "        text_encoded.append(texto_codificado)\n",
    "    text_encoded=''.join(text_encoded)\n",
    "    return text_encoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codificar_texto_ascii_binario(texto):\n",
    "    texto_codificado = ''.join(format(ord(char), '08b') for char in texto)\n",
    "    return texto_codificado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codificar_texto_huffman(texto):\n",
    "    frequencies = Counter(texto)\n",
    "    symbolweights = [(char, weight) for char, weight in frequencies.items()]\n",
    "    codigo_huffman = huffman.codebook(symbolweights)\n",
    "    texto_codificado = ''.join(codigo_huffman[char] for char in texto)\n",
    "    return texto_codificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'101010 || 111 || 100 || 01011 || 111 || 0010 || 1011 || 0100 || 111 || 00000 || 100 || 01011 || 1100 || 1101 || 1100 || 100 || 0001 || 111 || 10100 || 1100 || 0100 || 00001 || 010101 || 111 || 1011 || 0001 || 111 || 010100 || 0010 || 1011 || 10100 || 1101 || 0011 || 101011 || 0110 || 0011 || 100 || 01111 || 1101 || 0011 || 111 || 100 || 01111 || 1011 || 01110 || 0100 || 111 || 0100 || 00001 || 0011 || 111 || 1011 || 0001 || 1100 || 0110 || 1100 || 0010 || 100 || 1101 || 111 || 1101 || 100 || 0010 || 0110 || 01110 || 100 || 0110'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huffman_encoded=codificar_texto_huffman('I am not familiar with, or knowledgeable about the original languag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01001001 || 00100000 || 01100001 || 01101101 || 00100000 || 01101110 || 01101111 || 01110100 || 00100000 || 01100110 || 01100001 || 01101101 || 01101001 || 01101100 || 01101001 || 01100001 || 01110010 || 00100000 || 01110111 || 01101001 || 01110100 || 01101000 || 00101100 || 00100000 || 01101111 || 01110010 || 00100000 || 01101011 || 01101110 || 01101111 || 01110111 || 01101100 || 01100101 || 01100100 || 01100111 || 01100101 || 01100001 || 01100010 || 01101100 || 01100101 || 00100000 || 01100001 || 01100010 || 01101111 || 01110101 || 01110100 || 00100000 || 01110100 || 01101000 || 01100101 || 00100000 || 01101111 || 01110010 || 01101001 || 01100111 || 01101001 || 01101110 || 01100001 || 01101100 || 00100000 || 01101100 || 01100001 || 01101110 || 01100111 || 01110101 || 01100001 || 01100111'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ascii_encoded=codificar_texto_ascii_binario(\"I am not familiar with, or knowledgeable about the original languag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto a predecir: \n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar: I\n",
      "Texto codificado: 011001\n",
      "Texto a predecir: I\n",
      "Siguiente caracter más probable: N\n",
      "Caracter a codificar:  \n",
      "Texto codificado: 011\n",
      "Texto a predecir: I \n",
      "Siguiente caracter más probable: (\n",
      "Caracter a codificar: a\n",
      "Texto codificado: 0001110111\n",
      "Texto a predecir: I a\n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar: m\n",
      "Texto codificado: 001001010\n",
      "Texto a predecir: I am\n",
      "Siguiente caracter más probable: L\n",
      "Caracter a codificar:  \n",
      "Texto codificado: 111\n",
      "Texto a predecir: I am \n",
      "Siguiente caracter más probable: g\n",
      "Caracter a codificar: n\n",
      "Texto codificado: 1011000\n",
      "Texto a predecir: I am n\n",
      "Siguiente caracter más probable: o\n",
      "Caracter a codificar: o\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am no\n",
      "Siguiente caracter más probable: n\n",
      "Caracter a codificar: t\n",
      "Texto codificado: 11\n",
      "Texto a predecir: I am not\n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar:  \n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not \n",
      "Siguiente caracter más probable: i\n",
      "Caracter a codificar: f\n",
      "Texto codificado: 10111\n",
      "Texto a predecir: I am not f\n",
      "Siguiente caracter más probable: o\n",
      "Caracter a codificar: a\n",
      "Texto codificado: 100\n",
      "Texto a predecir: I am not fa\n",
      "Siguiente caracter más probable: c\n",
      "Caracter a codificar: m\n",
      "Texto codificado: 0000101\n",
      "Texto a predecir: I am not fam\n",
      "Siguiente caracter más probable: e\n",
      "Caracter a codificar: i\n",
      "Texto codificado: 1100\n",
      "Texto a predecir: I am not fami\n",
      "Siguiente caracter más probable: n\n",
      "Caracter a codificar: l\n",
      "Texto codificado: 00011\n",
      "Texto a predecir: I am not famil\n",
      "Siguiente caracter más probable: l\n",
      "Caracter a codificar: i\n",
      "Texto codificado: 1111\n",
      "Texto a predecir: I am not famili\n",
      "Siguiente caracter más probable: t\n",
      "Caracter a codificar: a\n",
      "Texto codificado: 00100\n",
      "Texto a predecir: I am not familia\n",
      "Siguiente caracter más probable: l\n",
      "Caracter a codificar: r\n",
      "Texto codificado: 111000\n",
      "Texto a predecir: I am not familiar\n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar:  \n",
      "Texto codificado: 11\n",
      "Texto a predecir: I am not familiar \n",
      "Siguiente caracter más probable: t\n",
      "Caracter a codificar: w\n",
      "Texto codificado: 10011\n",
      "Texto a predecir: I am not familiar w\n",
      "Siguiente caracter más probable: h\n",
      "Caracter a codificar: i\n",
      "Texto codificado: 00\n",
      "Texto a predecir: I am not familiar wi\n",
      "Siguiente caracter más probable: t\n",
      "Caracter a codificar: t\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar wit\n",
      "Siguiente caracter más probable: h\n",
      "Caracter a codificar: h\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with\n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar: ,\n",
      "Texto codificado: 00110\n",
      "Texto a predecir: I am not familiar with,\n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar:  \n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, \n",
      "Siguiente caracter más probable: a\n",
      "Caracter a codificar: o\n",
      "Texto codificado: 0000\n",
      "Texto a predecir: I am not familiar with, o\n",
      "Siguiente caracter más probable: r\n",
      "Caracter a codificar: r\n",
      "Texto codificado: 11\n",
      "Texto a predecir: I am not familiar with, or\n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar:  \n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or \n",
      "Siguiente caracter más probable: t\n",
      "Caracter a codificar: k\n",
      "Texto codificado: 11100011\n",
      "Texto a predecir: I am not familiar with, or k\n",
      "Siguiente caracter más probable: n\n",
      "Caracter a codificar: n\n",
      "Texto codificado: 0\n",
      "Texto a predecir: I am not familiar with, or kn\n",
      "Siguiente caracter más probable: o\n",
      "Caracter a codificar: o\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or kno\n",
      "Siguiente caracter más probable: w\n",
      "Caracter a codificar: w\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or know\n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar: l\n",
      "Texto codificado: 110\n",
      "Texto a predecir: I am not familiar with, or knowl\n",
      "Siguiente caracter más probable: e\n",
      "Caracter a codificar: e\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowle\n",
      "Siguiente caracter más probable: d\n",
      "Caracter a codificar: d\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowled\n",
      "Siguiente caracter más probable: g\n",
      "Caracter a codificar: g\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledg\n",
      "Siguiente caracter más probable: e\n",
      "Caracter a codificar: e\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledge\n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar: a\n",
      "Texto codificado: 001110\n",
      "Texto a predecir: I am not familiar with, or knowledgea\n",
      "Siguiente caracter más probable: l\n",
      "Caracter a codificar: b\n",
      "Texto codificado: 100\n",
      "Texto a predecir: I am not familiar with, or knowledgeab\n",
      "Siguiente caracter más probable: l\n",
      "Caracter a codificar: l\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeabl\n",
      "Siguiente caracter más probable: e\n",
      "Caracter a codificar: e\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable\n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar:  \n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable \n",
      "Siguiente caracter más probable: o\n",
      "Caracter a codificar: a\n",
      "Texto codificado: 000\n",
      "Texto a predecir: I am not familiar with, or knowledgeable a\n",
      "Siguiente caracter más probable: n\n",
      "Caracter a codificar: b\n",
      "Texto codificado: 1101\n",
      "Texto a predecir: I am not familiar with, or knowledgeable ab\n",
      "Siguiente caracter más probable: o\n",
      "Caracter a codificar: o\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable abo\n",
      "Siguiente caracter más probable: u\n",
      "Caracter a codificar: u\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable abou\n",
      "Siguiente caracter más probable: t\n",
      "Caracter a codificar: t\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about\n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar:  \n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about \n",
      "Siguiente caracter más probable: t\n",
      "Caracter a codificar: t\n",
      "Texto codificado: 01\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about t\n",
      "Siguiente caracter más probable: h\n",
      "Caracter a codificar: h\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about th\n",
      "Siguiente caracter más probable: e\n",
      "Caracter a codificar: e\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the\n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar:  \n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the \n",
      "Siguiente caracter más probable: p\n",
      "Caracter a codificar: o\n",
      "Texto codificado: 11111\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the o\n",
      "Siguiente caracter más probable: n\n",
      "Caracter a codificar: r\n",
      "Texto codificado: 10\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the or\n",
      "Siguiente caracter más probable: i\n",
      "Caracter a codificar: i\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the ori\n",
      "Siguiente caracter más probable: g\n",
      "Caracter a codificar: g\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the orig\n",
      "Siguiente caracter más probable: i\n",
      "Caracter a codificar: i\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the origi\n",
      "Siguiente caracter más probable: n\n",
      "Caracter a codificar: n\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the origin\n",
      "Siguiente caracter más probable: a\n",
      "Caracter a codificar: a\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the origina\n",
      "Siguiente caracter más probable: l\n",
      "Caracter a codificar: l\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original\n",
      "Siguiente caracter más probable:  \n",
      "Caracter a codificar:  \n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original \n",
      "Siguiente caracter más probable: o\n",
      "Caracter a codificar: l\n",
      "Texto codificado: 1110111\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original l\n",
      "Siguiente caracter más probable: i\n",
      "Caracter a codificar: a\n",
      "Texto codificado: 01\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original la\n",
      "Siguiente caracter más probable: w\n",
      "Caracter a codificar: n\n",
      "Texto codificado: 1010\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original lan\n",
      "Siguiente caracter más probable: g\n",
      "Caracter a codificar: g\n",
      "Texto codificado: 1\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original lang\n",
      "Siguiente caracter más probable: e\n",
      "Caracter a codificar: u\n",
      "Texto codificado: 0110\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original langu\n",
      "Siguiente caracter más probable: a\n",
      "Caracter a codificar: a\n",
      "Texto codificado: 11\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original langua\n",
      "Siguiente caracter más probable: l\n",
      "Caracter a codificar: g\n",
      "Texto codificado: 0110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'011001011000111011100100101011110110001111101111000000101110000011111100100111000111001100110011010000111111000110111101111001110100111000110111110111111111101111111111011101101010110110110'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptative_encoded=codificar_texto(\"I am not familiar with, or knowledgeable about the original languag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodificar_texto_huffman(texto_codificado, codigo_huffman):\n",
    "    codigo_inverso = {v: k for k, v in codigo_huffman.items()}\n",
    "    \n",
    "    texto_decodificado = ''\n",
    "    temp = ''\n",
    "    for bit in texto_codificado:\n",
    "        temp += bit\n",
    "        if temp in codigo_inverso:\n",
    "            texto_decodificado += codigo_inverso[temp]\n",
    "            temp = ''\n",
    "    \n",
    "    return texto_decodificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodificador_model(texto_encoded:str):\n",
    "    texto_decoded = ''\n",
    "    temp = ''\n",
    "    codigo_inverso_tmp = {}\n",
    "    for i in range(len(texto_encoded)):\n",
    "        if not temp:\n",
    "            input_text = ''\n",
    "            if i>0:\n",
    "                input_text=texto_decoded[:i]\n",
    "            print('Texto a predecir:',input_text)\n",
    "            predictions = predict_next_char(model, tokenizer, input_text, seq_length)\n",
    "            max_key = max(predictions, key=predictions.get)\n",
    "            print('Siguiente caracter más probable:',max_key)\n",
    "            symbolweights = [(char, weight) for char, weight in predictions.items()]\n",
    "            codigo_huffman = huffman.codebook(symbolweights)\n",
    "            codigo_inverso_tmp = {v: k for k, v in codigo_huffman.items()}\n",
    "        temp += texto_encoded[i]\n",
    "        if temp in codigo_inverso_tmp:\n",
    "            texto_decoded += codigo_inverso_tmp[temp]\n",
    "            temp = ''\n",
    "    return texto_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto a predecir: \n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I\n",
      "Siguiente caracter más probable: N\n",
      "Texto a predecir: I \n",
      "Siguiente caracter más probable: (\n",
      "Texto a predecir: I a\n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I am\n",
      "Siguiente caracter más probable: L\n",
      "Texto a predecir: I am \n",
      "Siguiente caracter más probable: g\n",
      "Texto a predecir: I am n\n",
      "Siguiente caracter más probable: o\n",
      "Texto a predecir: I am no\n",
      "Siguiente caracter más probable: n\n",
      "Texto a predecir: I am not\n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I am not \n",
      "Siguiente caracter más probable: i\n",
      "Texto a predecir: I am not f\n",
      "Siguiente caracter más probable: o\n",
      "Texto a predecir: I am not fa\n",
      "Siguiente caracter más probable: c\n",
      "Texto a predecir: I am not fam\n",
      "Siguiente caracter más probable: e\n",
      "Texto a predecir: I am not fami\n",
      "Siguiente caracter más probable: n\n",
      "Texto a predecir: I am not famil\n",
      "Siguiente caracter más probable: l\n",
      "Texto a predecir: I am not famili\n",
      "Siguiente caracter más probable: t\n",
      "Texto a predecir: I am not familia\n",
      "Siguiente caracter más probable: l\n",
      "Texto a predecir: I am not familiar\n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I am not familiar \n",
      "Siguiente caracter más probable: t\n",
      "Texto a predecir: I am not familiar w\n",
      "Siguiente caracter más probable: h\n",
      "Texto a predecir: I am not familiar wi\n",
      "Siguiente caracter más probable: t\n",
      "Texto a predecir: I am not familiar wit\n",
      "Siguiente caracter más probable: h\n",
      "Texto a predecir: I am not familiar with\n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I am not familiar with,\n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I am not familiar with, \n",
      "Siguiente caracter más probable: a\n",
      "Texto a predecir: I am not familiar with, o\n",
      "Siguiente caracter más probable: r\n",
      "Texto a predecir: I am not familiar with, or\n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I am not familiar with, or \n",
      "Siguiente caracter más probable: t\n",
      "Texto a predecir: I am not familiar with, or k\n",
      "Siguiente caracter más probable: n\n",
      "Texto a predecir: I am not familiar with, or kn\n",
      "Siguiente caracter más probable: o\n",
      "Texto a predecir: I am not familiar with, or kno\n",
      "Siguiente caracter más probable: w\n",
      "Texto a predecir: I am not familiar with, or know\n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I am not familiar with, or knowl\n",
      "Siguiente caracter más probable: e\n",
      "Texto a predecir: I am not familiar with, or knowle\n",
      "Siguiente caracter más probable: d\n",
      "Texto a predecir: I am not familiar with, or knowled\n",
      "Siguiente caracter más probable: g\n",
      "Texto a predecir: I am not familiar with, or knowledg\n",
      "Siguiente caracter más probable: e\n",
      "Texto a predecir: I am not familiar with, or knowledge\n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I am not familiar with, or knowledgea\n",
      "Siguiente caracter más probable: l\n",
      "Texto a predecir: I am not familiar with, or knowledgeab\n",
      "Siguiente caracter más probable: l\n",
      "Texto a predecir: I am not familiar with, or knowledgeabl\n",
      "Siguiente caracter más probable: e\n",
      "Texto a predecir: I am not familiar with, or knowledgeable\n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I am not familiar with, or knowledgeable \n",
      "Siguiente caracter más probable: o\n",
      "Texto a predecir: I am not familiar with, or knowledgeable a\n",
      "Siguiente caracter más probable: n\n",
      "Texto a predecir: I am not familiar with, or knowledgeable ab\n",
      "Siguiente caracter más probable: o\n",
      "Texto a predecir: I am not familiar with, or knowledgeable abo\n",
      "Siguiente caracter más probable: u\n",
      "Texto a predecir: I am not familiar with, or knowledgeable abou\n",
      "Siguiente caracter más probable: t\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about\n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I am not familiar with, or knowledgeable about \n",
      "Siguiente caracter más probable: t\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about t\n",
      "Siguiente caracter más probable: h\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about th\n",
      "Siguiente caracter más probable: e\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the\n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the \n",
      "Siguiente caracter más probable: p\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the o\n",
      "Siguiente caracter más probable: n\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the or\n",
      "Siguiente caracter más probable: i\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the ori\n",
      "Siguiente caracter más probable: g\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the orig\n",
      "Siguiente caracter más probable: i\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the origi\n",
      "Siguiente caracter más probable: n\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the origin\n",
      "Siguiente caracter más probable: a\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the origina\n",
      "Siguiente caracter más probable: l\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original\n",
      "Siguiente caracter más probable:  \n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original \n",
      "Siguiente caracter más probable: o\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original l\n",
      "Siguiente caracter más probable: i\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original la\n",
      "Siguiente caracter más probable: w\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original lan\n",
      "Siguiente caracter más probable: g\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original lang\n",
      "Siguiente caracter más probable: e\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original langu\n",
      "Siguiente caracter más probable: a\n",
      "Texto a predecir: I am not familiar with, or knowledgeable about the original langua\n",
      "Siguiente caracter más probable: l\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not familiar with, or knowledgeable about the original languag'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_encoded = '011001011000111011100100101011110110001111101111000000101110000011111100100111000111001100110011010000111111000110111101111001110100111000110111110111111111101111111111011101101010110110110'\n",
    "decodificador_model(texto_encoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fundamentos_matematicos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
