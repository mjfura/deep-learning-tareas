{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import huffman\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "import tensorflow_datasets as tfds\n",
    "import glob\n",
    "import os\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"este es un ejemplo de codificación de Huffman\"\n",
    "frequencies = Counter(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto codificado: 110 || 0100 || 111101 || 110 || 101 || 110 || 0100 || 101 || 0010 || 0111 || 101 || 110 || 111100 || 110 || 0011 || 00010 || 01010 || 0000 || 101 || 1110 || 110 || 101 || 0110 || 0000 || 1110 || 1001 || 1000 || 1001 || 0110 || 11111 || 0110 || 1001 || 00011 || 0111 || 101 || 1110 || 110 || 101 || 01011 || 0010 || 1000 || 1000 || 0011 || 11111 || 0111\n",
      "Texto decodificado: e\n"
     ]
    }
   ],
   "source": [
    "codigo_huffman = huffman.codebook(frequencies.items())\n",
    "texto_codificado = \" || \".join(codigo_huffman[char] for char in texto)\n",
    "\n",
    "print(\"Texto codificado:\", texto_codificado)\n",
    "\n",
    "# Paso 4: Decodificar el texto codificado\n",
    "# Crear un diccionario inverso para decodificar\n",
    "codigo_inverso = {v: k for k, v in codigo_huffman.items()}\n",
    "texto_decodificado = \"\"\n",
    "temp = \"\"\n",
    "for bit in texto_codificado:\n",
    "    temp += bit\n",
    "    if temp in codigo_inverso:\n",
    "        texto_decodificado += codigo_inverso[temp]\n",
    "        temp = \"\"\n",
    "\n",
    "print(\"Texto decodificado:\", texto_decodificado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, seq_length=40):\n",
    "    chars = sorted(set(text))\n",
    "    char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "    idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "    # Crear secuencias de entrada y salida\n",
    "    input_seq = []\n",
    "    target_seq = []\n",
    "\n",
    "    for i in range(len(text) - seq_length):\n",
    "        in_seq = text[i : i + seq_length]\n",
    "        out_seq = text[i + seq_length]\n",
    "        input_seq.append([char_to_idx[char] for char in in_seq])\n",
    "        target_seq.append(char_to_idx[out_seq])\n",
    "\n",
    "    # Convertir listas de índices en matrices de entrada y salida adecuadas para el entrenamiento\n",
    "    input_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        input_seq, maxlen=seq_length, padding=\"pre\"\n",
    "    )\n",
    "    target_seq = tf.keras.utils.to_categorical(target_seq, num_classes=vocab_size)\n",
    "\n",
    "    return input_seq, target_seq, vocab_size, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape: [[ 1  0  4 ... 15  8 16]\n",
      " [ 0  4  1 ...  8 16  1]\n",
      " [ 4  1  7 ... 16  1 22]\n",
      " ...\n",
      " [ 1 14  9 ... 18 18  5]\n",
      " [14  9  1 ... 18  5  6]\n",
      " [ 9  1  5 ...  5  6  5]]\n",
      "Target sequence shape: [[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "Vocabulary size: {0: '\\n', 1: ' ', 2: 'A', 3: 'P', 4: 'Y', 5: 'a', 6: 'b', 7: 'c', 8: 'd', 9: 'e', 10: 'g', 11: 'h', 12: 'i', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'z', 24: 'á', 25: 'é', 26: 'í'}\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcofura/anaconda3/envs/fundamentos_matematicos/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 - 1s - 132ms/step - accuracy: 0.1618 - loss: 3.2793\n",
      "Epoch 2/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.1863 - loss: 3.1925\n",
      "Epoch 3/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.1863 - loss: 2.9225\n",
      "Epoch 4/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.1912 - loss: 2.8323\n",
      "Epoch 5/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.1422 - loss: 2.7818\n",
      "Epoch 6/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.2157 - loss: 2.7413\n",
      "Epoch 7/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.1863 - loss: 2.7347\n",
      "Epoch 8/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.1863 - loss: 2.7149\n",
      "Epoch 9/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.1912 - loss: 2.6929\n",
      "Epoch 10/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.1961 - loss: 2.6745\n",
      "Epoch 11/100\n",
      "7/7 - 0s - 13ms/step - accuracy: 0.1961 - loss: 2.6526\n",
      "Epoch 12/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.1961 - loss: 2.6338\n",
      "Epoch 13/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.2451 - loss: 2.5988\n",
      "Epoch 14/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.2500 - loss: 2.5682\n",
      "Epoch 15/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.2941 - loss: 2.5319\n",
      "Epoch 16/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.3088 - loss: 2.4894\n",
      "Epoch 17/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.3039 - loss: 2.4495\n",
      "Epoch 18/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.3333 - loss: 2.3798\n",
      "Epoch 19/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.3578 - loss: 2.3306\n",
      "Epoch 20/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.3676 - loss: 2.2633\n",
      "Epoch 21/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.3922 - loss: 2.2106\n",
      "Epoch 22/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.3725 - loss: 2.1578\n",
      "Epoch 23/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.4069 - loss: 2.0736\n",
      "Epoch 24/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.4167 - loss: 2.0253\n",
      "Epoch 25/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.4216 - loss: 1.9771\n",
      "Epoch 26/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.4314 - loss: 1.8975\n",
      "Epoch 27/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.4804 - loss: 1.8377\n",
      "Epoch 28/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.4853 - loss: 1.7812\n",
      "Epoch 29/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.4853 - loss: 1.7178\n",
      "Epoch 30/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.5245 - loss: 1.6404\n",
      "Epoch 31/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.5196 - loss: 1.5937\n",
      "Epoch 32/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.5147 - loss: 1.5406\n",
      "Epoch 33/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.5539 - loss: 1.4793\n",
      "Epoch 34/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.5833 - loss: 1.4181\n",
      "Epoch 35/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.6127 - loss: 1.3527\n",
      "Epoch 36/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.6029 - loss: 1.2925\n",
      "Epoch 37/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.6078 - loss: 1.2499\n",
      "Epoch 38/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.6618 - loss: 1.2247\n",
      "Epoch 39/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.6618 - loss: 1.1522\n",
      "Epoch 40/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.7304 - loss: 1.0821\n",
      "Epoch 41/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.7451 - loss: 1.0135\n",
      "Epoch 42/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.7255 - loss: 0.9525\n",
      "Epoch 43/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.7990 - loss: 0.9007\n",
      "Epoch 44/100\n",
      "7/7 - 0s - 15ms/step - accuracy: 0.7843 - loss: 0.8587\n",
      "Epoch 45/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.8333 - loss: 0.7930\n",
      "Epoch 46/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.8480 - loss: 0.7196\n",
      "Epoch 47/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.8676 - loss: 0.6747\n",
      "Epoch 48/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.8775 - loss: 0.6248\n",
      "Epoch 49/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.8725 - loss: 0.5987\n",
      "Epoch 50/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9363 - loss: 0.5435\n",
      "Epoch 51/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.9363 - loss: 0.4977\n",
      "Epoch 52/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9608 - loss: 0.4630\n",
      "Epoch 53/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9657 - loss: 0.4255\n",
      "Epoch 54/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9804 - loss: 0.3951\n",
      "Epoch 55/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9706 - loss: 0.3769\n",
      "Epoch 56/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9657 - loss: 0.3671\n",
      "Epoch 57/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9853 - loss: 0.3156\n",
      "Epoch 58/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.9951 - loss: 0.2924\n",
      "Epoch 59/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 0.9902 - loss: 0.2699\n",
      "Epoch 60/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9951 - loss: 0.2432\n",
      "Epoch 61/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 0.9902 - loss: 0.2288\n",
      "Epoch 62/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9902 - loss: 0.2154\n",
      "Epoch 63/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9951 - loss: 0.1990\n",
      "Epoch 64/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9951 - loss: 0.1811\n",
      "Epoch 65/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 0.9951 - loss: 0.1661\n",
      "Epoch 66/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.1540\n",
      "Epoch 67/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.1433\n",
      "Epoch 68/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 1.0000 - loss: 0.1316\n",
      "Epoch 69/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.1232\n",
      "Epoch 70/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.1153\n",
      "Epoch 71/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.1079\n",
      "Epoch 72/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.1018\n",
      "Epoch 73/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0957\n",
      "Epoch 74/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0914\n",
      "Epoch 75/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0869\n",
      "Epoch 76/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0828\n",
      "Epoch 77/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0796\n",
      "Epoch 78/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0757\n",
      "Epoch 79/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0712\n",
      "Epoch 80/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0678\n",
      "Epoch 81/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0653\n",
      "Epoch 82/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0626\n",
      "Epoch 83/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0596\n",
      "Epoch 84/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0573\n",
      "Epoch 85/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0547\n",
      "Epoch 86/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0527\n",
      "Epoch 87/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0502\n",
      "Epoch 88/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0488\n",
      "Epoch 89/100\n",
      "7/7 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0469\n",
      "Epoch 90/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0457\n",
      "Epoch 91/100\n",
      "7/7 - 0s - 14ms/step - accuracy: 1.0000 - loss: 0.0440\n",
      "Epoch 92/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0424\n",
      "Epoch 93/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0414\n",
      "Epoch 94/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0396\n",
      "Epoch 95/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0385\n",
      "Epoch 96/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 1.0000 - loss: 0.0374\n",
      "Epoch 97/100\n",
      "7/7 - 0s - 9ms/step - accuracy: 1.0000 - loss: 0.0362\n",
      "Epoch 98/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0348\n",
      "Epoch 99/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0344\n",
      "Epoch 100/100\n",
      "7/7 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3130e3590>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de ejemplo\n",
    "text = \"\"\" Hola, ¿cómo estás? Soy un ejemplo de texto que se utilizará para entrenar un modelo de lenguaje.\n",
    "\"\"\"  \n",
    "seq_length = 10  \n",
    "input_seq, target_seq, vocab_size, char_to_idx, idx_to_char = preprocess_text(\n",
    "    text, seq_length\n",
    ")\n",
    "print(\"Input sequence shape:\", input_seq)\n",
    "print(\"Target sequence shape:\", target_seq)\n",
    "print(\"Vocabulary size:\", idx_to_char)\n",
    "# Construir el modelo\n",
    "model = Sequential(\n",
    "    [\n",
    "        Embedding(input_dim=vocab_size, output_dim=64, input_length=seq_length),\n",
    "        LSTM(128),\n",
    "        Dense(vocab_size, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(input_seq, target_seq, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "{'\\n': 2.4446226e-06, ' ': 0.00012292077, 'A': 3.9709444e-06, 'P': 0.00018478041, 'Y': 0.00021594099, 'a': 0.002025231, 'b': 0.0045404113, 'c': 2.0484175e-07, 'd': 4.2705764e-05, 'e': 0.9816928, 'g': 2.1880491e-07, 'h': 2.7613967e-05, 'i': 0.000121175595, 'l': 0.0043668956, 'm': 0.0023769338, 'n': 2.608391e-05, 'o': 5.560564e-06, 'q': 2.5820671e-05, 'r': 0.0002562776, 's': 0.0029980824, 't': 2.9368462e-05, 'u': 0.00015539383, 'v': 0.00076011376, 'z': 7.896544e-07, 'á': 5.8225937e-06, 'é': 1.0626007e-05, 'í': 1.7690279e-06}\n"
     ]
    }
   ],
   "source": [
    "def predict_next_char(model, input_text, char_to_idx, idx_to_char):\n",
    "    input_indices = [\n",
    "        char_to_idx[ch] for ch in input_text[-seq_length:]\n",
    "    ]  # Asegurarse de usar solo los últimos 'seq_length' caracteres\n",
    "    input_indices = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [input_indices], maxlen=seq_length, padding=\"pre\"\n",
    "    )\n",
    "    predictions = model.predict(input_indices)[0]\n",
    "    return {idx_to_char[i]: prob for i, prob in enumerate(predictions)}\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "context = \"[]Hola\"\n",
    "predicted_probs = predict_next_char(model, context, char_to_idx, idx_to_char)\n",
    "print(predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts(directory):\n",
    "    texts = []\n",
    "    # Recorre todos los archivos en el directorio\n",
    "    for filename in glob.glob(os.path.join(directory, \"*\")):\n",
    "        if os.path.isfile(filename):  # Check if the path is a file\n",
    "            with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "                texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Suponiendo que '20news-bydate-train' es tu directorio descomprimido\n",
    "train_dir = \"./20news-bydate/20news-bydate-train/alt.atheism\"\n",
    "test_dir = \"./20news-bydate/20news-bydate-test/alt.atheism\"\n",
    "texts = load_texts(train_dir)\n",
    "texts_test = load_texts(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(texts, seq_length=40):\n",
    "    input_chars = []\n",
    "    output_char = []\n",
    "    for text in texts:\n",
    "        for i in range(len(text) - seq_length):\n",
    "            in_seq = text[i:i + seq_length]\n",
    "            out_seq = text[i + seq_length]\n",
    "            input_chars.append(in_seq)\n",
    "            output_char.append(out_seq)\n",
    "    return input_chars, output_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 30\n",
    "texts_train_part = texts[:100]\n",
    "texts_test_part = texts_test[:25]\n",
    "input_seqs, output_seqs = create_sequences(texts_train_part, seq_length)\n",
    "input_seqs_test, output_seqs_test = create_sequences(texts_test_part, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_20 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_21 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 66ms/step - accuracy: 0.2129 - loss: 2.9440 - val_accuracy: 0.3511 - val_loss: 2.3590\n",
      "Epoch 2/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 59ms/step - accuracy: 0.3700 - loss: 2.2322 - val_accuracy: 0.4063 - val_loss: 2.1578\n",
      "Epoch 3/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 65ms/step - accuracy: 0.4254 - loss: 2.0310 - val_accuracy: 0.4390 - val_loss: 2.0505\n",
      "Epoch 4/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 68ms/step - accuracy: 0.4568 - loss: 1.9027 - val_accuracy: 0.4585 - val_loss: 1.9800\n",
      "Epoch 5/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 69ms/step - accuracy: 0.4769 - loss: 1.8230 - val_accuracy: 0.4678 - val_loss: 1.9428\n",
      "Epoch 6/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 73ms/step - accuracy: 0.4931 - loss: 1.7608 - val_accuracy: 0.4755 - val_loss: 1.9131\n",
      "Epoch 7/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 73ms/step - accuracy: 0.5042 - loss: 1.7200 - val_accuracy: 0.4823 - val_loss: 1.8926\n",
      "Epoch 8/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 73ms/step - accuracy: 0.5140 - loss: 1.6772 - val_accuracy: 0.4916 - val_loss: 1.8824\n",
      "Epoch 9/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 73ms/step - accuracy: 0.5226 - loss: 1.6465 - val_accuracy: 0.4965 - val_loss: 1.8522\n",
      "Epoch 10/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 73ms/step - accuracy: 0.5268 - loss: 1.6279 - val_accuracy: 0.4977 - val_loss: 1.8537\n",
      "Epoch 11/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 73ms/step - accuracy: 0.5306 - loss: 1.6062 - val_accuracy: 0.5022 - val_loss: 1.8456\n",
      "Epoch 12/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 68ms/step - accuracy: 0.5385 - loss: 1.5763 - val_accuracy: 0.5051 - val_loss: 1.8464\n",
      "Epoch 13/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 75ms/step - accuracy: 0.5432 - loss: 1.5635 - val_accuracy: 0.5115 - val_loss: 1.8181\n",
      "Epoch 14/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 74ms/step - accuracy: 0.5457 - loss: 1.5538 - val_accuracy: 0.5098 - val_loss: 1.8312\n",
      "Epoch 15/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 72ms/step - accuracy: 0.5476 - loss: 1.5372 - val_accuracy: 0.5096 - val_loss: 1.8247\n",
      "Epoch 16/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 70ms/step - accuracy: 0.5524 - loss: 1.5257 - val_accuracy: 0.5148 - val_loss: 1.8098\n",
      "Epoch 17/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 66ms/step - accuracy: 0.5558 - loss: 1.5105 - val_accuracy: 0.5194 - val_loss: 1.8072\n",
      "Epoch 18/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 64ms/step - accuracy: 0.5553 - loss: 1.5045 - val_accuracy: 0.5169 - val_loss: 1.8145\n",
      "Epoch 19/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 66ms/step - accuracy: 0.5581 - loss: 1.4994 - val_accuracy: 0.5201 - val_loss: 1.8043\n",
      "Epoch 20/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 70ms/step - accuracy: 0.5642 - loss: 1.4822 - val_accuracy: 0.5214 - val_loss: 1.8009\n",
      "Epoch 21/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 71ms/step - accuracy: 0.5663 - loss: 1.4714 - val_accuracy: 0.5178 - val_loss: 1.8040\n",
      "Epoch 22/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 68ms/step - accuracy: 0.5664 - loss: 1.4681 - val_accuracy: 0.5182 - val_loss: 1.7985\n",
      "Epoch 23/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 69ms/step - accuracy: 0.5656 - loss: 1.4686 - val_accuracy: 0.5221 - val_loss: 1.7986\n",
      "Epoch 24/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 67ms/step - accuracy: 0.5688 - loss: 1.4631 - val_accuracy: 0.5182 - val_loss: 1.8011\n",
      "Epoch 25/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 71ms/step - accuracy: 0.5706 - loss: 1.4495 - val_accuracy: 0.5173 - val_loss: 1.8065\n",
      "Epoch 26/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 73ms/step - accuracy: 0.5707 - loss: 1.4471 - val_accuracy: 0.5213 - val_loss: 1.8045\n",
      "Epoch 27/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 72ms/step - accuracy: 0.5722 - loss: 1.4434 - val_accuracy: 0.5257 - val_loss: 1.7935\n",
      "Epoch 28/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 68ms/step - accuracy: 0.5744 - loss: 1.4367 - val_accuracy: 0.5220 - val_loss: 1.8049\n",
      "Epoch 29/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 70ms/step - accuracy: 0.5746 - loss: 1.4386 - val_accuracy: 0.5253 - val_loss: 1.8084\n",
      "Epoch 30/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 70ms/step - accuracy: 0.5736 - loss: 1.4315 - val_accuracy: 0.5248 - val_loss: 1.7943\n",
      "Epoch 31/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 69ms/step - accuracy: 0.5756 - loss: 1.4296 - val_accuracy: 0.5255 - val_loss: 1.7956\n",
      "Epoch 32/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 64ms/step - accuracy: 0.5779 - loss: 1.4164 - val_accuracy: 0.5229 - val_loss: 1.8057\n",
      "Epoch 33/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 64ms/step - accuracy: 0.5789 - loss: 1.4150 - val_accuracy: 0.5246 - val_loss: 1.8068\n",
      "Epoch 34/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 71ms/step - accuracy: 0.5770 - loss: 1.4207 - val_accuracy: 0.5264 - val_loss: 1.8108\n",
      "Epoch 35/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 70ms/step - accuracy: 0.5809 - loss: 1.4169 - val_accuracy: 0.5229 - val_loss: 1.8040\n",
      "Epoch 36/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 72ms/step - accuracy: 0.5785 - loss: 1.4121 - val_accuracy: 0.5282 - val_loss: 1.7930\n",
      "Epoch 37/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 71ms/step - accuracy: 0.5792 - loss: 1.4095 - val_accuracy: 0.5279 - val_loss: 1.8100\n",
      "Epoch 38/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 72ms/step - accuracy: 0.5812 - loss: 1.4005 - val_accuracy: 0.5291 - val_loss: 1.8040\n",
      "Epoch 39/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 73ms/step - accuracy: 0.5845 - loss: 1.3979 - val_accuracy: 0.5293 - val_loss: 1.7902\n",
      "Epoch 40/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 74ms/step - accuracy: 0.5819 - loss: 1.3964 - val_accuracy: 0.5294 - val_loss: 1.7935\n",
      "Epoch 41/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 70ms/step - accuracy: 0.5863 - loss: 1.3891 - val_accuracy: 0.5303 - val_loss: 1.7993\n",
      "Epoch 42/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 73ms/step - accuracy: 0.5856 - loss: 1.3896 - val_accuracy: 0.5280 - val_loss: 1.8108\n",
      "Epoch 43/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 72ms/step - accuracy: 0.5858 - loss: 1.3891 - val_accuracy: 0.5283 - val_loss: 1.8039\n",
      "Epoch 44/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 66ms/step - accuracy: 0.5873 - loss: 1.3854 - val_accuracy: 0.5259 - val_loss: 1.8047\n",
      "Epoch 45/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 76ms/step - accuracy: 0.5883 - loss: 1.3816 - val_accuracy: 0.5278 - val_loss: 1.7973\n",
      "Epoch 46/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 66ms/step - accuracy: 0.5871 - loss: 1.3797 - val_accuracy: 0.5288 - val_loss: 1.8007\n",
      "Epoch 47/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 71ms/step - accuracy: 0.5884 - loss: 1.3796 - val_accuracy: 0.5260 - val_loss: 1.8038\n",
      "Epoch 48/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 71ms/step - accuracy: 0.5877 - loss: 1.3805 - val_accuracy: 0.5274 - val_loss: 1.8109\n",
      "Epoch 49/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 80ms/step - accuracy: 0.5919 - loss: 1.3740 - val_accuracy: 0.5277 - val_loss: 1.8042\n",
      "Epoch 50/50\n",
      "\u001b[1m2681/2681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 70ms/step - accuracy: 0.5899 - loss: 1.3761 - val_accuracy: 0.5291 - val_loss: 1.8089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14890f230>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Tokenización\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(texts+texts_test)  # Ajuste del tokenizer al corpus completo\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convertir textos a secuencias numéricas\n",
    "input_seqs = tokenizer.texts_to_sequences(input_seqs)\n",
    "input_seqs = pad_sequences(input_seqs, maxlen=seq_length, truncating='pre')\n",
    "\n",
    "# Preparar datos de salida (etiquetas) usando one-hot encoding\n",
    "output_seqs = tokenizer.texts_to_sequences(output_seqs)\n",
    "output_seqs = to_categorical(output_seqs, num_classes=vocab_size)\n",
    "\n",
    "# Test\n",
    "input_seqs_test = tokenizer.texts_to_sequences(input_seqs_test)\n",
    "input_seqs_test = pad_sequences(input_seqs_test, maxlen=seq_length, truncating='pre')\n",
    "\n",
    "# Preparar datos de salida (etiquetas) usando one-hot encoding\n",
    "output_seqs_test = tokenizer.texts_to_sequences(output_seqs_test)\n",
    "output_seqs_test = to_categorical(output_seqs_test, num_classes=vocab_size)\n",
    "\n",
    "# Construcción del modelo LSTM\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=50, input_length=seq_length),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(128),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model.fit(input_seqs, output_seqs, epochs=50, batch_size=64,validation_data=(input_seqs_test, output_seqs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar texto de entrada \n",
    "def codificar_texto(texto:str):\n",
    "    texto_codificado = tokenizer.texts_to_sequences([texto])[0]\n",
    "    texto_codificado = pad_sequences([texto_codificado], maxlen=seq_length, truncating='pre')\n",
    "    return texto_codificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 1.5950792e-10, ' ': 0.012281137, 'e': 8.72832e-05, 't': 0.0036291692, 'i': 2.0899139e-05, 'a': 0.00014589165, 'o': 0.00045588036, 's': 0.22694807, 'n': 0.37558985, 'r': 0.014647618, 'h': 7.445102e-05, 'l': 0.0008568549, 'd': 0.035022456, 'u': 0.0016275678, '\\n': 0.0018883744, 'c': 0.0009687037, 'm': 0.011669023, '.': 0.027806953, 'y': 9.848865e-06, 'f': 0.15203448, 'p': 0.00027516356, 'g': 0.0010468533, 'w': 4.818458e-05, 'b': 0.0007719995, '>': 1.5344362e-05, 'v': 0.0024651545, ',': 0.00018973678, '-': 4.819494e-06, 'k': 0.0017686542, ':': 1.6008191e-09, \"'\": 0.015728982, '\"': 2.8503813e-08, '*': 1.258957e-08, 'j': 1.6582248e-06, ')': 0.00017777098, '(': 0.00013986464, '1': 2.6450449e-05, '@': 1.101738e-05, 'x': 0.00051517796, '^': 8.9487695e-13, '?': 7.769665e-06, '0': 6.468009e-05, 'z': 5.496394e-07, '|': 1.7282577e-05, '2': 2.8803897e-05, 'q': 4.2125785e-06, '3': 0.110196695, '=': 4.4372505e-06, '9': 3.362148e-05, '/': 9.07967e-07, '5': 3.551089e-05, '4': 6.4445226e-06, '#': 2.530649e-16, '_': 9.72431e-07, '6': 5.7078727e-05, '<': 5.6791464e-08, '\\\\': 5.70926e-09, '8': 7.036653e-06, '7': 2.2644452e-05, '!': 0.00047962405, '\\t': 3.827207e-06, '[': 3.3741156e-11, ']': 9.1371913e-07, ';': 1.4913146e-08, '+': 4.9014045e-05, '~': 1.8756488e-10, '$': 2.6495893e-05, '`': 2.1223765e-12, '&': 2.874829e-13, '%': 1.0637939e-08, '}': 1.4393309e-10, '{': 1.0634987e-10, '\\x1b': 1.2210202e-10, '\\x0c': 1.9242015e-10, '\\x10': 9.316267e-11}\n"
     ]
    }
   ],
   "source": [
    "def predict_next_char(model, tokenizer, input_text, seq_length):\n",
    "    # Paso 1: Convertir la cadena de entrada en una secuencia de enteros\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    \n",
    "    # Paso 2: Ajustar la longitud de la secuencia\n",
    "    input_seq = pad_sequences([input_seq], maxlen=seq_length, truncating='pre')\n",
    "    \n",
    "    # Paso 3: Hacer la predicción\n",
    "    prediction = model.predict(input_seq, verbose=0)[0]\n",
    "    \n",
    "    # Paso 4: Convertir las probabilidades en un diccionario\n",
    "    int_to_char = {i: char for char, i in tokenizer.word_index.items()}\n",
    "    int_to_char[0] = ''  # Añadir el carácter vacío para el padding\n",
    "    return {int_to_char[i]: prob for i, prob in enumerate(prediction)}\n",
    "\n",
    "# Ejemplo de uso\n",
    "input_text = \"this i\"\n",
    "predictions = predict_next_char(model, tokenizer, input_text, seq_length)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key con mayor valor: n\n"
     ]
    }
   ],
   "source": [
    "max_key = max(predictions, key=predictions.get)\n",
    "print(\"Key con mayor valor:\", max_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fundamentos_matematicos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
